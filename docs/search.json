[
  {
    "objectID": "course-materials/labs/lab2.html",
    "href": "course-materials/labs/lab2.html",
    "title": "Lab 2",
    "section": "",
    "text": "In today’s lab you will…\n\nFit a linear model to real world data\nInterpret model coefficients\nSimulate data according to model assumptions and recover parameters\n\n\nlibrary(tidyverse)\ntheme_set(theme_bw(18))"
  },
  {
    "objectID": "course-materials/labs/lab2.html#learning-objectives",
    "href": "course-materials/labs/lab2.html#learning-objectives",
    "title": "Lab 2",
    "section": "",
    "text": "In today’s lab you will…\n\nFit a linear model to real world data\nInterpret model coefficients\nSimulate data according to model assumptions and recover parameters\n\n\nlibrary(tidyverse)\ntheme_set(theme_bw(18))"
  },
  {
    "objectID": "course-materials/labs/lab2.html#storms-and-floods",
    "href": "course-materials/labs/lab2.html#storms-and-floods",
    "title": "Lab 2",
    "section": "Storms and floods",
    "text": "Storms and floods\nWe’ll begin with an example of the relationship between precipitation and flooding during storms in LA. Save the LA Storm Data CSV to your data/ folder and use it to fill in the following code chunk. Each row in the CSV file contains data from a storm in LA during the last fifteen years. gage_ht is the height of the Los Angeles River at the Sepulveda Dam (in feet) and precip_wk_in is the total precipitation measured during the previous week at the Hollywood Burbank Airport. You can find data like these through USGS and NCEI, respectively.\n\n# Read the CSV file into a data frame\n\n# Create a plot of the gage height and precipitation data\n\nQ1: Which variables did you choose for the x- and y-axis? What does this imply about the data generating process?\nIn Quarto documents, we can add statistical notation using LaTeX. We’ll fill in the equation below to describe the model.\n\\[\n\\begin{align}\n??? &\\sim Normal(???, ???) \\\\\n??? &= ??? + ???\n\\end{align}\n\\]\n\nFit model\nThe lm() function fits a linear regression model to data. It has two important parameters: the formula defining the model, and the data frame containing the data.\nOnce you’ve fit the model, use summary() to see the details, including the coefficients and \\(R^2\\) value. \\(\\hat \\sigma\\) is in there too, but doesn’t print by default. Assign the result of summary() to a variable called storm_summ and try to find the standard deviation from there (hint: it’s an element in that variable).\nQ2: What are the estimates for \\(\\hat \\beta_0, \\hat \\beta_1, \\hat \\sigma\\) ?\n\n\nInterpret model\nQ3: In plain language, describe how you’d interpret \\(\\beta_0\\) and \\(\\beta_1\\). What do they represent?\nLet’s also interpret \\(R^2\\), the measure of model fit. Recall \\(R^2\\) describes the amount of variance explained by the model. In other words, how much variance did we “explain away” using the model’s prediction line. In equations, it looks like this.\n\\[\n\\begin{align}\nSST &= \\sum_i (y_i - \\bar y)^2 \\\\\nSSE &= \\sum_i (y_i - \\hat y_i)^2 \\\\\nR^2 &= \\frac{SST-SSE}{SST}\n\\end{align}\n\\]\nWe can get \\(R^2\\) directly from the model summary in R, but it’s helpful for your learning to do it a couple times directly, too. Calculate \\(SST\\), \\(SSE\\), and \\(R^2\\) arithmetically in the code chunk below. You can use the coef() function to get the model coefficient estimates, but don’t use residuals().\n\ny_bar &lt;- ???\ny_hat &lt;- ???\nSST &lt;- ???\nSSE &lt;- ???\nR2 &lt;- ???\n\nQ4: What’s the difference between \\(y\\), \\(\\bar y\\), and \\(\\hat y\\)?\nQ5: What does the \\(R^2\\) value tell you about the quality of the model (in plain language)?\nRecall that the purpose of a model is to explain variance.\nQ6: What’s the estimated value of \\(\\hat \\sigma\\) for the model? What’s the value of \\(\\sigma\\) for the response variable itself? Say you needed to predict Los Angeles River gage height following a storm - what does the relationship between these two numbers say about your ability to make that prediction accurately?"
  },
  {
    "objectID": "course-materials/labs/lab2.html#simulating-data-from-a-model",
    "href": "course-materials/labs/lab2.html#simulating-data-from-a-model",
    "title": "Lab 2",
    "section": "Simulating data from a model",
    "text": "Simulating data from a model\nSimulating data from a model is a super power for your learning! This is a great tool for unambiguously testing your understanding. Here’s the basic process:\n\nRead the model description in statistical notation\nChoose a set of parameters and predictor variable(s) for your simulation\nUse a random variable to generate a response, based on your parameters and predictor(s)\nFit a model to your simulated data\nCheck if your model’s parameter estimates match what you chose\n\nMastering this process will give you a massive boost in learning new statistical tools!\nLet’s do it for linear regression.\n\n1. Read the model description\nHere’s the description of a linear regression model in statistical notation.\n\\[\n\\begin{align}\ny &\\sim Normal(\\mu, \\sigma) \\\\\n\\mu &= \\beta_0 + \\beta_1 x\n\\end{align}\n\\]\n\n\n2. Choose parameters, predictor(s)\nWe have three parameters in our model: \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma\\). You can choose any valid value for them.\n\nbeta_0 &lt;- ???\nbeta_1 &lt;- ???\nsigma &lt;- ???\n\nChoose any valid value for x, too. In linear regression, the predictor can take any real value. Let’s choose 100 numbers evenly spaced between 0 and 10. You could generate these randomly, too, but you don’t have to.\n\nx &lt;- ???\n\n\n\n3. Generate response\nCheck out our model description - it tells us what to do next. \\(\\mu\\) is calculated directly from \\(\\beta_0\\), \\(\\beta_1\\), and \\(x\\), which we just chose. \\(y\\) is distributed as a normal variable with mean \\(\\mu\\) (which we calculate) and standard deviation \\(\\sigma\\) (which we chose). Is distributed as is our clue we need to create a random variable.\n\nmu &lt;- ???\ny &lt;- rnorm(n = ???, mean = ???, sd = ???)\n\n\n\n4. Fit the model\nAs we did above.\n\nsim_dat &lt;- tibble(???)\nsim_mod &lt;- lm(??? ~ ???, data = sim_dat)\n\n\n\n5. Check your model\nDo the model parameter estimates match our chosen simulation parameters?\n\nmod_summ &lt;- summary(???)\nbeta_0_hat &lt;- ???\nbeta_1_hat &lt;- ???\nsigma_hat &lt;- ???\n\nTry changing your chosen parameters for your model and see if you can recover them. As long as you keep your simulated data large enough (n &gt; 100) you should get good results.\nAs we start to use more complicated models, this procedure will become invaluable. The key thing about it is you can check your work: if you’re consistently getting the wrong estimates for your parameters then you know you have an error in your understanding. That’s what office hours are for!"
  },
  {
    "objectID": "course-materials/labs/lab2.html#rockfish-survey-gaps",
    "href": "course-materials/labs/lab2.html#rockfish-survey-gaps",
    "title": "Lab 2",
    "section": "Rockfish survey gaps",
    "text": "Rockfish survey gaps\nIn the rest of this lab, you’ll apply what you learned in the previous examples to fill gaps in a marine ecosystem survey.\nThe Rockfish Recruitment and Ecosystem Assessment Survey (RREAS) is a long-running survey of juvenile rockfish, their physical environment, and their predators along the US west coast. The survey is vitally important for understanding the population dynamics of economically and culturally important species, such as bocaccio. RREAS data inform fishery management decisions to protect threatened populations while supporting California’s coastal economy. For more information about RREAS, see their Story Map.\n\n\n\nThe US West Coast Groundfish Fishery harvested 1 million pounds of bocaccio in 2023, valued at $650k. Source: NOAA Fisheries\n\n\nThe RREAS has run almost every spring for over 40 years. The “almost” qualifier is necessary because the COVID-19 pandemic greatly reduced survey effort in 2020. NOAA scientists used linear regression to fill the gap in their dataset with a proxy measurement: diets of common murres.\n\n\n\nA conceptual model of the dynamics governing murres, their prey, and the environment.\n\n\nThe common murre is a seabird that breeds in large numbers off the coast of California. In spring, pairs of murres raise one chick by bringing it juvenile fish, including rockfish, anchovy, and salmon. The composition of common murre diets on the Farallon Islands (~30 miles west of San Francisco) has been recorded by biologists with Point Blue Conservation since the 1970s.\n\nExplore the data\nRead the data from the rreas.csv file and create a scatter plot relating fish abundance indices to common murre diet.\n\nrreas_murres &lt;- read_csv(\"data/rreas.csv\")\n\nQ6: Which variables did you choose for the x- and y-axis? What does this mean for the purpose of your model?\n\n\nFit the model\nFit a model that predicts the RREAS rockfish abundance index from the fraction of rockfish in common murre diets.\n\n\nInterpret the model\nFind the \\(R^2\\) value for your model.\nQ7: What does the R2 value tell you about the utility of your model for filling in the survey gap?\nExtract the estimated coefficients (\\(\\hat \\beta_0, \\hat \\beta_1\\)) and standard deviation (\\(\\hat \\sigma\\)) from the model.\nQ8: In plain language, what do the coefficients and standard deviation of the model represent?\nSay it’s 2020 and the RREAS has largely been canceled. You ask Point Blue to share their common murre diet data with you and they say it was 32.8% juvenile rockfish. Estimate the predicted value of the juvenile rockfish abundance index.\nQ9: What did you predict the juvenile rockfish abundance index was in 2020?\nQ10: Your prediction is the most likely outcome, but the actual index won’t take that value exactly. How would you estimate an interval that you could confidently say probably contains the actual index?"
  },
  {
    "objectID": "course-materials/labs/lab2.html#recap",
    "href": "course-materials/labs/lab2.html#recap",
    "title": "Lab 2",
    "section": "Recap",
    "text": "Recap\nIn this lab, you fit linear regression models to one predictor and one response variable. You used statistical notation to describe the model, which makes two things unambiguous.\n\nIt shows the relationship between the predictor, model coefficients, and the expected value of the response for a given value of the predictor.\nIt says the observed response values are normally distributed around the expected value, with a standard deviation of \\(\\sigma\\).\n\nIn future weeks, we’ll build on this foundation to make more complicated models. You’ll add additional predictors, transform parameters to introduce non-linearity, and use non-normal random variables to model other types of responses."
  },
  {
    "objectID": "course-materials/labs/lab4-key.html",
    "href": "course-materials/labs/lab4-key.html",
    "title": "Lab 4 (key)",
    "section": "",
    "text": "In today’s lab you will…\n\nRun a permutation test to answer a “yes/no?” question\nBootstrap a confidence interval to answer a “how much?” question\n\n\nlibrary(tidyverse)\ntheme_set(theme_classic(14))\nset.seed(123)"
  },
  {
    "objectID": "course-materials/labs/lab4-key.html#learning-objectives",
    "href": "course-materials/labs/lab4-key.html#learning-objectives",
    "title": "Lab 4 (key)",
    "section": "",
    "text": "In today’s lab you will…\n\nRun a permutation test to answer a “yes/no?” question\nBootstrap a confidence interval to answer a “how much?” question\n\n\nlibrary(tidyverse)\ntheme_set(theme_classic(14))\nset.seed(123)"
  },
  {
    "objectID": "course-materials/labs/lab4-key.html#eel-grass-restoration",
    "href": "course-materials/labs/lab4-key.html#eel-grass-restoration",
    "title": "Lab 4 (key)",
    "section": "Eel grass restoration",
    "text": "Eel grass restoration\nRecall the eel grass restoration example from yesterday’s lecture. The data recorded whether attempts to restore eel grass were successful based on the method used (garden staple or popsicle stick) Figure 1.\n\nrestoration &lt;- read_csv(\"data/eelgrass.csv\",\n                        show_col_types = FALSE)\nglimpse(restoration)\n\nRows: 100\nColumns: 5\n$ plot_id          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ treatment        &lt;chr&gt; \"Garden staple\", \"Popsicle stick\", \"Garden staple\", \"…\n$ success          &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,…\n$ shoot_density_m2 &lt;dbl&gt; 45.94241, 91.60585, 120.92760, 67.86314, 88.09977, 87…\n$ success_fct      &lt;chr&gt; \"Succeed\", \"Succeed\", \"Fail\", \"Fail\", \"Succeed\", \"Suc…\n\n\n\nggplot(restoration, aes(treatment, fill = success_fct)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"firebrick\", \"cornflowerblue\")) +\n  labs(x = \"Restoration method\",\n       y = \"Sites\",\n       fill = \"Outcome\") +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(1, 1),\n        legend.justification = c(1, 1))\n\n\n\n\n\n\n\nFigure 1: Success of eelgrass restoration plots by restoration method."
  },
  {
    "objectID": "course-materials/labs/lab4-key.html#permutation-test",
    "href": "course-materials/labs/lab4-key.html#permutation-test",
    "title": "Lab 4 (key)",
    "section": "Permutation test",
    "text": "Permutation test\nWe want to know if the popsicle stick method works better for restoration than garden staples. This is a “yes/no?” question, so we’ll use a permutation test for our hypothesis. Recall the steps for hypothesis testing:\n\nIdentify the TEST STATISTIC\nState your NULL and ALTERNATIVE hypotheses\nCalculate the OBSERVED test statistic\nEstimate the NULL DISTRIBUTION\nCalculate P-VALUE\nCompare p-value to CRITICAL THRESHOLD\n\n\nIdentify the test statistic\nQ1: What is the appropriate test statistic for this question?\nDifference in proportions\n\n\nState your null and alternative hypotheses\nQ2: What are your null and alternative hypotheses?\nH0: There’s no difference in outcome between garden staple and popsicle stick methods.\nHA: The popsicle stick method is more likely to succeed than the garden staple method.\n\n\nCalculate the observed test statistic\nQ3: How would you calculate the test statistic for the sample?\n\nsuccess_props &lt;- restoration %&gt;% \n  group_by(treatment) %&gt;% \n  summarize(prop_success = mean(success))\n\ndiff_props &lt;- success_props$prop_success[2] - success_props$prop_success[1]\n\ndiff_props\n\n[1] 0.1416667\n\n\n\n\nEstimate the null distribution\nThis is the key part of a permutation test! Remember, our goal is to estimate the distribution of possible outcomes under the null hypothesis. To do that, we have to break the association between treatment and outcome.\nQ4: What column should we shuffle to break the association between treatment and outcome?\ntreatment\nQ5: Fill in the following code to perform one permutation and calculate the test statistic.\n\none_permutation &lt;- restoration %&gt;% \n  mutate(treatment = sample(treatment, \n                            size = length(treatment), \n                            replace = FALSE))\n\npermutation_props &lt;- one_permutation %&gt;% \n  group_by(treatment) %&gt;% \n  summarize(prop_success = mean(success))\n\npermutation_diff_props &lt;- permutation_props$prop_success[2] - permutation_props$prop_success[1]\n\npermutation_diff_props\n\n[1] -0.06666667\n\n\nThat gives us the value of the test statistic for just one permutation. To get a distribution, we have to repeat the process many times. Let’s do it 1,000 times.\nQ6: Fill in the following code to perform 1,000 permutations and estimate the null distribution\n\npermute &lt;- function(i) {\n  one_permutation &lt;- restoration %&gt;% \n    mutate(treatment = sample(treatment, \n                              size = length(treatment), \n                              replace = FALSE))\n  \n  permutation_props &lt;- one_permutation %&gt;% \n    group_by(treatment) %&gt;% \n    summarize(prop_success = mean(success))\n  \n  permutation_diff_props &lt;- permutation_props$prop_success[2] - permutation_props$prop_success[1]\n  \n  permutation_diff_props\n}\n\nnull_distribution &lt;- map_dbl(1:1000, permute)\n\nQ7: Visualize the null distribution using a histogram and show where the observed test statistic falls\n\nnull_distribution_df &lt;- tibble(null_distribution)\nggplot(null_distribution_df, aes(null_distribution)) +\n  geom_histogram(binwidth = 0.05) +\n  geom_vline(xintercept = diff_props, \n             color = \"firebrick\",\n             linewidth = 1.5)\n\n\n\n\n\n\n\n\n\n\nCalculate the p-value\nThe p-value is the probability of a test statistic at least as extreme as the observed, given the null hypothesis. In other words, what proportion of the null distribution exceeds the observed?\nQ8: Calculate the p-value using the null distribution and observed test statistic\n\np_val &lt;- mean(null_distribution &gt; diff_props)\n\np_val\n\n[1] 0.059\n\n# Note: the in-class p-value was 0.045. This p-value is 0.059. That's due to variation in the permutation sampling, a difference of ~1%.\n\n\n\nInterpret the p-value\nWhen interpreting the p-value, we compare it to a critical threshold, usually denoted with \\(\\alpha\\). By convention, we usually set \\(\\alpha\\) to 0.05.\nQ10: Given \\(p \\gt \\alpha\\), which of the following statements is a correct interpretation and why?\n\nOur evidence is consistent with the hypothesis that restoration method does not influence restoration outcome\nWe cannot reject the hypothesis that restoration method does not influence restoration outcome\n\nThe second interpretation is correct, because it simply fails to reject the null hypothesis. The first interpretation is incorrect because it implies we found evidence to support the null hypothesis, which is too strong of a statement given the results."
  },
  {
    "objectID": "course-materials/labs/lab4-key.html#bootstrap-confidence-interval",
    "href": "course-materials/labs/lab4-key.html#bootstrap-confidence-interval",
    "title": "Lab 4 (key)",
    "section": "Bootstrap confidence interval",
    "text": "Bootstrap confidence interval\nNow let’s answer a “how much?” question. We want to estimate an interval that we think contains the population parameter. For that, we use bootstrapping.\nRecall the steps for bootstrapping:\n\nIdentify the TEST STATISTIC\nSubstitute sample for population and draw BOOTSTRAP SAMPLES\nEstimate the BOOTSTRAP DISTRIBUTION of the test statistic\nCalculate the CONFIDENCE INTERVAL\n\n\nIdentify the test statistic\nQ11: What is the appropriate test statistic for this question?\nDifference in proportions\n\n\nDraw bootstrap samples\nThis is the key part of bootstrapping! Remember, our goal is to estimate the variation of our population’s parameter due to sampling. To do that, we simulate the process of re-doing our experiment, using the original sample as a substitute for the population.\nQ12: Fill in the following code to draw one bootstrap sample.\n\none_bootstrap &lt;- restoration %&gt;% \n  group_by(treatment) %&gt;% \n  mutate(success = sample(success,\n                          size = length(success),\n                          replace = TRUE)) %&gt;% \n  ungroup()\n\nQ13: Fill in the following code to draw 1,000 bootstrap samples.\n\n\n\n\n\n\nTip\n\n\n\nlist_rbind() will take a list of data frames and bind them row-wise into one data frame.\n\n\n\nbootstrap &lt;- function(i) {\n  one_bootstrap &lt;- restoration %&gt;% \n    group_by(treatment) %&gt;% \n    mutate(success = sample(success,\n                            size = length(success),\n                            replace = TRUE)) %&gt;% \n    ungroup() %&gt;% \n    mutate(trial = i)\n}\n\nbootstrap_samples &lt;- map(1:1000, bootstrap) %&gt;% \n  list_rbind()\n\n\n\nEstimate the bootstrap distribution of the test statistic\nQ14: Fill in the following code to alculate the test statistic for each bootstrap sample.\n\nboot_diff_prop &lt;- bootstrap_samples %&gt;% \n  group_by(trial, treatment) %&gt;% \n  summarize(prop_success = mean(success),\n            .groups = \"drop_last\") %&gt;% \n  summarize(diff_prop = prop_success[2] - prop_success[1])\n\nQ15: Visualize the bootstrapped distribution of the test statistic.\n\nggplot(boot_diff_prop, aes(diff_prop)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nCalculate the confidence interval\nA confidence interval (CI) is a range we are confident contains the population parameter. The bootstrapped distribution of the test statistic describes where we expect the population parameter to fall. So a 95% confidence interval, for example, spans the range from the 2.5% quantile of the bootstrap distribution to the 97.5% quantile.\nQ16: Find the bounds of the 95% CI.\n\n\n\n\n\n\nTip\n\n\n\nThe quantile() function finds quantiles. It’s vectorized over the parameter probs, so you can find multiple quantiles at once\n\n\n\nrestoration_ci &lt;- quantile(boot_diff_prop$diff_prop, c(0.025, 0.975))\nrestoration_ci\n\n       2.5%       97.5% \n-0.05020833  0.34166667 \n\n\nQ17: Update your visual from Q15 to include the observed test statistic with a solid line and the confidence interval represented with dotted lines.\n\nggplot(boot_diff_prop, aes(diff_prop)) +\n  geom_histogram() +\n  geom_vline(xintercept = diff_props, \n             color = \"firebrick\",\n             linewidth = 1.5) +\n  geom_vline(xintercept = restoration_ci, \n             color = \"firebrick\",\n             linetype = \"dotted\",\n             linewidth = 1.5)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "course-materials/labs/lab4-key.html#permutation-vs-bootstrap",
    "href": "course-materials/labs/lab4-key.html#permutation-vs-bootstrap",
    "title": "Lab 4 (key)",
    "section": "Permutation vs bootstrap",
    "text": "Permutation vs bootstrap\nThe visualization you created for Q7 shows the null distribution of the test statistic. The visualization you created for Q17 shows the bootstrapped distribution of the test statistic.\nQ18: What did you do to make the null distribution center on zero? Specifically, what code?\nBroke the association between treatment and outcome. Sampled treatment without replacement.\nQ19: What did you do to make the bootstrap distribution center on the observed test statistic? Specifically, what code?\nRetained the association between treatment and outcome. Grouped by treatment, then sampled with replacement.\nQ20: What would happen to your bootstrap distribution if you sampled without replacement?\nSampling without replacement is just shuffling. Since we sampled within treatments, the bootstrap samples would always match the observed sample. So there would be no variation in the bootstrap distribution of the test statistic."
  },
  {
    "objectID": "course-materials/lecture-slides/scratch/week2/rain_flooding.html",
    "href": "course-materials/lecture-slides/scratch/week2/rain_flooding.html",
    "title": "rain_flooding",
    "section": "",
    "text": "library(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr   1.1.4     ✔ readr   2.1.5\n✔ forcats 1.0.0     ✔ stringr 1.5.1\n✔ ggplot2 3.5.1     ✔ tibble  3.2.1\n✔ purrr   1.0.2     ✔ tidyr   1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(1234)\ntheme_set(theme_classic(18))\n\nGage height\n\n# https://waterdata.usgs.gov/monitoring-location/USGS-11092450/#dataTypeId=continuous-00065--695467286&showFieldMeasurements=true&startDT=2010-01-01&endDT=2025-10-01\ngage_colnames &lt;- c(\"agency_cd\", \"site_no\", \"datetime\", \"tz_cd\", \"gage_ht\", \"qualtiy_code\")\ngage_ht &lt;- read_tsv(\"~/Downloads/sepulveda_dam_2010_2025.tsv\", skip = 29, col_names = gage_colnames)\n\nRows: 549696 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (3): agency_cd, tz_cd, qualtiy_code\ndbl  (2): site_no, gage_ht\ndttm (1): datetime\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmax_winter_ht &lt;- gage_ht %&gt;% \n  mutate(yr = year(datetime),\n         mo = month(datetime)) %&gt;% \n  filter(mo %in% 1:2) %&gt;% \n  group_by(yr) %&gt;% \n  filter(gage_ht == max(gage_ht)) %&gt;% \n  slice_tail(n = 1) %&gt;% \n  ungroup()\n\nPrecipitation\n\nprecip &lt;- read_csv(\"~/Downloads/data.csv\", skip = 1) %&gt;% \n  mutate(yr = year(Date),\n         mo = month(Date)) %&gt;% \n  filter(mo %in% c(12, 1, 2), yr %in% 2009:2025) %&gt;% \n  select(Date, precip_in = `PRCP (Inches)`)\n\nRows: 9991 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (6): TAVG (Degrees Fahrenheit), TMAX (Degrees Fahrenheit), TMIN (Degree...\ndate (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nMerge\n\nweekly_precip &lt;- function(stormdatetime) {\n  precip %&gt;% \n    filter(between(Date, stormdatetime - weeks(1), stormdatetime)) %&gt;% \n    pull(precip_in) %&gt;% \n    sum(na.rm = TRUE)\n}\nstorm_data &lt;- max_winter_ht %&gt;% \n  mutate(precip_wk_in = map_dbl(datetime, weekly_precip)) %&gt;% \n  select(yr, datetime, gage_ht, precip_wk_in)\n\nPlot\n\nstorm_data %&gt;% \n  filter(precip_wk_in &lt; 5) %&gt;% \n  ggplot(aes(precip_wk_in, gage_ht)) +\n  geom_point(shape = 21, color = \"firebrick\", stroke = 2) +\n  ylim(0, NA) +\n  labs(x = \"Total precip. prev. week (in)\",\n       y = \"Gage height (ft)\")\n\n\n\n\n\n\n\n\nWith two lines, which is better?\n\nstorm_data %&gt;% \n  filter(precip_wk_in &lt; 5) %&gt;% \n  ggplot(aes(precip_wk_in, gage_ht)) +\n  geom_point(shape = 21, color = \"firebrick\", stroke = 2) +\n  geom_abline(intercept = 2, slope = 3, color = \"cornflowerblue\", linewidth = 2, linetype = \"dotted\") +\n  geom_abline(intercept = 4, slope = 2, color = \"cornflowerblue\", linewidth = 2, linetype = \"dashed\") +\n  ylim(0, NA) +\n  labs(x = \"Total precip. prev. week (in)\",\n       y = \"Gage height (ft)\")\n\n\n\n\n\n\n\n\nResiduals\n\nstorm_data %&gt;% \n  filter(precip_wk_in &lt; 5) %&gt;% \n  mutate(resid1 = gage_ht - (2 + 3 * precip_wk_in),\n         resid2 = gage_ht - (4 + 2 * precip_wk_in)) %&gt;% \n  summarize(ssr1 = sum(resid1^2),\n            ssr2 = sum(resid2^2))\n\n# A tibble: 1 × 2\n   ssr1  ssr2\n  &lt;dbl&gt; &lt;dbl&gt;\n1  79.2  65.7\n\n\nResidual map\n\nresid_map &lt;- expand_grid(b = seq(2, 5, length.out = 20),\n            m = seq(1, 4, length.out = 20)) %&gt;% \n  mutate(ssr = map2_dbl(b, m, \\(.b, .m) {\n    storm_data %&gt;% \n      filter(precip_wk_in &lt; 5) %&gt;% \n      mutate(r = gage_ht - (.b + .m * precip_wk_in)) %&gt;% \n      summarize(ssr = sum(r^2)) %&gt;% \n      pull(ssr)\n  }))\nggplot(resid_map, aes(b, m, fill = ssr)) +\n  geom_tile() +\n  geom_point(data = arrange(resid_map, ssr) %&gt;% slice(1),\n             color = \"gold\") +\n  scale_fill_distiller(palette = \"Spectral\") +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n# remove the highest precip points\nstorm_data &lt;- filter(storm_data, precip_wk_in &lt; 5)\n\n\nstorm_mod &lt;- lm(gage_ht ~ precip_wk_in, data = storm_data)\nsummary(storm_mod)\n\n\nCall:\nlm(formula = gage_ht ~ precip_wk_in, data = storm_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6688 -1.1225 -0.6316  0.3928  4.7668 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.9133     1.1674   3.352  0.00576 **\nprecip_wk_in   2.1697     0.5645   3.844  0.00234 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.32 on 12 degrees of freedom\nMultiple R-squared:  0.5518,    Adjusted R-squared:  0.5144 \nF-statistic: 14.77 on 1 and 12 DF,  p-value: 0.002337\n\n\nVariation of the response\n\ngage_mean &lt;- mean(storm_data$gage_ht)\ngage_sd &lt;- sd(storm_data$gage_ht)\nggplot(storm_data, aes(gage_ht)) +\n  geom_histogram(bins = 6, fill = \"cornflowerblue\") +\n  geom_vline(xintercept = gage_mean,\n             color = \"firebrick\",\n             linewidth = 2) +\n  geom_segment(x = gage_mean - gage_sd,\n               xend = gage_mean + gage_sd,\n               y = 4, \n               yend = 4, \n               color = \"firebrick\",\n               linewidth = 2,\n               linetype = \"dotted\") +\n  labs(x = \"Gage height (ft)\")\n\n\n\n\n\n\n\n\nVariation of the residuals\n\nresidual_segments &lt;- storm_data %&gt;% \n  mutate(x = precip_wk_in,\n         xend = precip_wk_in,\n         y = gage_ht,\n         yend = predict(storm_mod))\nggplot(storm_data, aes(precip_wk_in, gage_ht)) +\n  geom_segment(aes(x = x, xend = xend, y = y , yend = yend),\n               residual_segments,\n               color = \"gold\",\n               linewidth = 1.5) +\n  geom_point(shape = 21, color = \"firebrick\", stroke = 2) +\n  geom_abline(intercept = coef(storm_mod)[1],\n              slope = coef(storm_mod)[2],\n              color = \"cornflowerblue\",\n              linewidth = 2) +\n  ylim(0, NA) +\n  labs(x = \"Total precip. prev. week (in)\",\n       y = \"Gage height (ft)\")\n\n\n\n\n\n\n\ngage_resid &lt;- tibble(residual = resid(storm_mod))\nresid_mean &lt;- mean(gage_resid$residual)\nresid_sd &lt;- sd(gage_resid$residual)\nggplot(gage_resid, aes(residual)) +\n  geom_histogram(bins = 6, fill = \"gold\") +\n  geom_vline(xintercept = resid_mean,\n             color = \"firebrick\",\n             linewidth = 2) +\n  geom_segment(x = resid_mean - resid_sd,\n               xend = resid_mean + resid_sd,\n               y = 4, \n               yend = 4, \n               color = \"firebrick\",\n               linewidth = 2,\n               linetype = \"dotted\") +\n  labs(x = \"Residual (ft)\")\n\n\n\n\n\n\n\n\n\\[\n\\begin{align}\nSST &= \\text{Total Sum of Squares} \\\\\nSST &= \\sum_i{(y_i - \\bar{y})^2} \\\\\nSSE &= \\text{Total Sum of Errors} \\\\\nSST &= \\sum_i{(y_i - \\hat{y})^2} \\\\\nR^2 &= \\frac{SST - SSE}{SST}\n\\end{align}\n\\]\nCaution!\n\nset.seed(123)\nsin_dat &lt;- tibble(\n  x = runif(20, 0, 300),\n  y = sin(x * pi / 180) + rnorm(20, 0, 0.15)\n)\nsin_mod &lt;- lm(y ~ x, sin_dat)\nsummary(sin_mod)\n\n\nCall:\nlm(formula = y ~ x, data = sin_dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.89938 -0.27910 -0.01412  0.41464  0.60438 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.237400   0.214755   5.762 1.84e-05 ***\nx           -0.007653   0.001136  -6.734 2.60e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4659 on 18 degrees of freedom\nMultiple R-squared:  0.7159,    Adjusted R-squared:  0.7001 \nF-statistic: 45.35 on 1 and 18 DF,  p-value: 2.596e-06\n\nggplot(sin_dat, aes(x, y)) +\n  geom_point(shape = 21, color = \"firebrick\", stroke = 2) +\n  geom_abline(intercept = coef(sin_mod)[1],\n              slope = coef(sin_mod)[2],\n              color = \"cornflowerblue\",\n              linewidth = 2) +\n  theme_classic(18)\n\n\n\n\n\n\n\nsin_dat %&gt;% \n  mutate(residual = resid(sin_mod)) %&gt;% \n  ggplot(aes(x, residual)) +\n  geom_hline(yintercept = 0, color = \"cornflowerblue\", linewidth = 2) +\n  geom_point(shape = 21, color = \"firebrick\", stroke = 2) +\n  theme_classic(18)\n\n\n\n\n\n\n\n\nSliding normal variable\n\nstorm_data %&gt;% \n  filter(precip_wk_in &lt; 5) %&gt;% \n  ggplot(aes(precip_wk_in, gage_ht)) +\n  geom_point(shape = 21, color = \"firebrick\", stroke = 2) +\n  geom_abline(intercept = coef(storm_mod)[1],\n              slope = coef(storm_mod)[2],\n              color = \"cornflowerblue\",\n              linewidth = 2) +\n  ylim(0, NA) +\n  labs(x = \"Total precip. prev. week (in)\",\n       y = \"Gage height (ft)\")"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Statistics for Environmental Data Science",
    "section": "Course Description",
    "text": "Course Description\nStatistics is the science of collecting, manipulating, and analyzing empirical data. In this class, we will learn the statistical fundamentals that will enable us to draw conclusions about the environment and its interaction with social and economic systems. We will cover fundamental statistical concepts and tools, and then apply and expand upon those tools to learn some temporal and spatial statistical methods that are particularly helpful in environmental data science. Welcome!\n\nLearning objectives\nThe goal of this course is to enable MEDS students to confidently and competently apply statistical tools to socio-environmental datasets. Essential skills and concepts are:\n\nSummarize data numerically and graphically\nInterpret random variables\nSimulate data to test assumptions\nDescribe models using statistical notation\nDraw DAGs to clarify causal relationships\nQuantify uncertainty to perform inference"
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Statistics for Environmental Data Science",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\n\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n\nMax Czapanskiy\nEmail: maxczap@ucsb.edu\n\n\n\n\n\nTA\n\n\n\n\n\n\n\n\n\n\n\nNathan Grimes\nEmail: ngrimes@bren.ucsb.edu"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Recommended reading & materials",
    "section": "",
    "text": "Consider adding helpful resources (e.g. online text books, blogs, people in the field to follow, videos, etc.) here. See EDS 240’s resources page as an example."
  },
  {
    "objectID": "course-materials/week9.html",
    "href": "course-materials/week9.html",
    "title": "Spatial and temporal methods",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nMidterm review\nNone\n\n\nLab\nMidterm 3\nNone\n\n\nLecture 2\nARIMA and kriging\nNone"
  },
  {
    "objectID": "course-materials/week9.html#lecture-materials",
    "href": "course-materials/week9.html#lecture-materials",
    "title": "Spatial and temporal methods",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nMidterm review\nNone\n\n\nLab\nMidterm 3\nNone\n\n\nLecture 2\nARIMA and kriging\nNone"
  },
  {
    "objectID": "course-materials/week9.html#assignment-reminders",
    "href": "course-materials/week9.html#assignment-reminders",
    "title": "Spatial and temporal methods",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\nAssignment Type\nNote\n\n\n\n\nHomework\nHomework 3 due\n\n\nFinal project\nFit model to real world data"
  },
  {
    "objectID": "course-materials/week7.html",
    "href": "course-materials/week7.html",
    "title": "Logistic regression",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nLogistic regression\nIMS Ch 9, 26\n\n\nLab\nNo class\nNone\n\n\nLecture 2\nColor of drinking water\nNone"
  },
  {
    "objectID": "course-materials/week7.html#lecture-materials",
    "href": "course-materials/week7.html#lecture-materials",
    "title": "Logistic regression",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nLogistic regression\nIMS Ch 9, 26\n\n\nLab\nNo class\nNone\n\n\nLecture 2\nColor of drinking water\nNone"
  },
  {
    "objectID": "course-materials/week7.html#assignment-reminders",
    "href": "course-materials/week7.html#assignment-reminders",
    "title": "Logistic regression",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\nAssignment Type\nNote\n\n\n\n\nHomework\nHomework 2 re-assessment due\n\n\nHomework\nHomework 3 posted\n\n\nFinal project\nDescribe hypotheses with text and visuals"
  },
  {
    "objectID": "course-materials/week5.html",
    "href": "course-materials/week5.html",
    "title": "Mechanics of Inference",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nAdventures in inference\nIMS Ch 18-21\n\n\nLab\nMathematical inference in R\nNone\n\n\nLecture 2\nLinear regression inference\nIMS Ch 24-25"
  },
  {
    "objectID": "course-materials/week5.html#lecture-materials",
    "href": "course-materials/week5.html#lecture-materials",
    "title": "Mechanics of Inference",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nAdventures in inference\nIMS Ch 18-21\n\n\nLab\nMathematical inference in R\nNone\n\n\nLecture 2\nLinear regression inference\nIMS Ch 24-25"
  },
  {
    "objectID": "course-materials/week5.html#assignment-reminders",
    "href": "course-materials/week5.html#assignment-reminders",
    "title": "Mechanics of Inference",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\nAssignment Type\nNote\n\n\n\n\nHomework\nHomework 2 posted\nHomework 1 re-assessment due\nMidterm 1 re-assessment due\n\n\nFinal project\nExploratory analysis due"
  },
  {
    "objectID": "course-materials/week3.html",
    "href": "course-materials/week3.html",
    "title": "Regression continued",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nInteractions\nNone\n\n\nLab\nMidterm review (Theme 1 Quizzes)\nNone\n\n\nLecture 2\nMidterm 1\nNone"
  },
  {
    "objectID": "course-materials/week3.html#lecture-materials",
    "href": "course-materials/week3.html#lecture-materials",
    "title": "Regression continued",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nInteractions\nNone\n\n\nLab\nMidterm review (Theme 1 Quizzes)\nNone\n\n\nLecture 2\nMidterm 1\nNone"
  },
  {
    "objectID": "course-materials/week3.html#assignment-reminders",
    "href": "course-materials/week3.html#assignment-reminders",
    "title": "Regression continued",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\nAssignment Type\nNote\n\n\n\n\nHomework\nHomework 1 due"
  },
  {
    "objectID": "course-materials/week10.html",
    "href": "course-materials/week10.html",
    "title": "Wrapping up",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nGuest research talk\nNone\n\n\nLab\nFinal project peer feedback\nNone\n\n\nLecture 2\nFinal project co-working time\nNone"
  },
  {
    "objectID": "course-materials/week10.html#lecture-materials",
    "href": "course-materials/week10.html#lecture-materials",
    "title": "Wrapping up",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nGuest research talk\nNone\n\n\nLab\nFinal project peer feedback\nNone\n\n\nLecture 2\nFinal project co-working time\nNone"
  },
  {
    "objectID": "course-materials/week10.html#assignment-reminders",
    "href": "course-materials/week10.html#assignment-reminders",
    "title": "Wrapping up",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\nAssignment Type\nNote\n\n\n\n\nHomework\nHomework 3 re-assessment due\n\n\nFinal project\nFirst draft of blog post due Thursday 12/4"
  },
  {
    "objectID": "course-materials/discussion-materials/week1-discussion-slides.html#title-slide",
    "href": "course-materials/discussion-materials/week1-discussion-slides.html#title-slide",
    "title": "EDS 222: Statistics for Environmental Data Science",
    "section": "",
    "text": "EDS 240: Discussion 1\nToday’s topic here\n\nWeek 1 | January 7th, 2024"
  },
  {
    "objectID": "course-materials/discussion-materials/week1-discussion-slides.html#slide-slug-here",
    "href": "course-materials/discussion-materials/week1-discussion-slides.html#slide-slug-here",
    "title": "EDS 222: Statistics for Environmental Data Science",
    "section": "",
    "text": "Slide title"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Homework",
    "section": "",
    "text": "Important: 11:59 PM submission deadline\n\n\n\nAll assignments are due at 11:59 PM on the dates listed in the table, below.\n\n\n\n\n\nCourse Theme\nDate Posted\nDate Due\nRe-assessment Deadline\n\n\n\n\nExploring uncertainty\n10/8\n10/17\n10/31\n\n\nConducting inference\n10/29\n11/7\n11/21\n\n\nNon-normal data\n11/12\n11/26\n12/5"
  },
  {
    "objectID": "course-materials/week1.html",
    "href": "course-materials/week1.html",
    "title": "Random Variables",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nPopulations, samples, and parameters\nIMS Ch 1-2\n\n\nLab\nExploratory data analysis\nIMS Ch 4-5\n\n\nLecture 2\nRandom variables and distribution functions\nNone"
  },
  {
    "objectID": "course-materials/week1.html#lecture-materials",
    "href": "course-materials/week1.html#lecture-materials",
    "title": "Random Variables",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nPopulations, samples, and parameters\nIMS Ch 1-2\n\n\nLab\nExploratory data analysis\nIMS Ch 4-5\n\n\nLecture 2\nRandom variables and distribution functions\nNone"
  },
  {
    "objectID": "course-materials/week1.html#assignment-reminders",
    "href": "course-materials/week1.html#assignment-reminders",
    "title": "Random Variables",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\nNothing due in week 1. In week 2, you’ll submit your choice of data and question for the final project. That can more time than you might expect, so it’s a good idea to start this week!"
  },
  {
    "objectID": "course-materials/week2.html",
    "href": "course-materials/week2.html",
    "title": "Regression",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nLinear regression\nIMS Ch 7\n\n\nLab\nRockfish survey gaps\nSantora et al. (2021) Nature Communications (optional)\n\n\nLecture 2\nMultiple regression\nIMS Ch 8"
  },
  {
    "objectID": "course-materials/week2.html#lecture-materials",
    "href": "course-materials/week2.html#lecture-materials",
    "title": "Regression",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nLinear regression\nIMS Ch 7\n\n\nLab\nRockfish survey gaps\nSantora et al. (2021) Nature Communications (optional)\n\n\nLecture 2\nMultiple regression\nIMS Ch 8"
  },
  {
    "objectID": "course-materials/week2.html#assignment-reminders",
    "href": "course-materials/week2.html#assignment-reminders",
    "title": "Regression",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\nAssignment Type\nNote\n\n\n\n\nHomework\nHomework 1 posted\n\n\nFinal project\nChoose data and question"
  },
  {
    "objectID": "course-materials/week4.html",
    "href": "course-materials/week4.html",
    "title": "Introduction to Inference",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nPermutation and bootstrapping\nIMS Ch 11-12\n\n\nLab\nPermutation and bootstrapping in R (key)\nNone\n\n\nLecture 2\nInference with mathematical models\nIMS Ch 13, 16-17"
  },
  {
    "objectID": "course-materials/week4.html#lecture-materials",
    "href": "course-materials/week4.html#lecture-materials",
    "title": "Introduction to Inference",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nPermutation and bootstrapping\nIMS Ch 11-12\n\n\nLab\nPermutation and bootstrapping in R (key)\nNone\n\n\nLecture 2\nInference with mathematical models\nIMS Ch 13, 16-17"
  },
  {
    "objectID": "course-materials/week4.html#assignment-reminders",
    "href": "course-materials/week4.html#assignment-reminders",
    "title": "Introduction to Inference",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\nAssignment Type\nNote\n\n\n\n\nHomework\nHomework 1 re-assessment due\nHomework 1 re-assessment due week 5\n\n\nFinal project\nExploratory analysis due\nExploratory analysis due week 5"
  },
  {
    "objectID": "course-materials/week6.html",
    "href": "course-materials/week6.html",
    "title": "Connections in inference",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nForks, pipes, and colliders\nNone\n\n\nLab\nMidterm review\nNone\n\n\nLecture 2\nMidterm 2\nNone"
  },
  {
    "objectID": "course-materials/week6.html#lecture-materials",
    "href": "course-materials/week6.html#lecture-materials",
    "title": "Connections in inference",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nForks, pipes, and colliders\nNone\n\n\nLab\nMidterm review\nNone\n\n\nLecture 2\nMidterm 2\nNone"
  },
  {
    "objectID": "course-materials/week6.html#assignment-reminders",
    "href": "course-materials/week6.html#assignment-reminders",
    "title": "Connections in inference",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\nAssignment Type\nNote\n\n\n\n\nHomework\nHomework 2 due"
  },
  {
    "objectID": "course-materials/week8.html",
    "href": "course-materials/week8.html",
    "title": "GLMs and autocorrelation",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nPoisson regression\nNone\n\n\nLab\nCalifornia wildfire occurrence\nNone\n\n\nLecture 2\nSpatial and temporal autocorrelation\nNone"
  },
  {
    "objectID": "course-materials/week8.html#lecture-materials",
    "href": "course-materials/week8.html#lecture-materials",
    "title": "GLMs and autocorrelation",
    "section": "",
    "text": "Session\nTopic\nReading\n\n\n\n\nLecture 1\nPoisson regression\nNone\n\n\nLab\nCalifornia wildfire occurrence\nNone\n\n\nLecture 2\nSpatial and temporal autocorrelation\nNone"
  },
  {
    "objectID": "course-materials/week8.html#assignment-reminders",
    "href": "course-materials/week8.html#assignment-reminders",
    "title": "GLMs and autocorrelation",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\n\nAssignment Type\nNote\n\n\n\n\nFinal project\nDescribe model in statistical notation and fit model to simulated data"
  },
  {
    "objectID": "final-project.html",
    "href": "final-project.html",
    "title": "Final Project",
    "section": "",
    "text": "In your career as an environmental data scientist, you will often have to learn new analytic approaches to solve problems. The process of learning an unfamiliar statistical model is an important skill on its own. In this final project, you will use the skills you’ve developed in class to teach yourself a statistical model and apply it to an environmental dataset."
  },
  {
    "objectID": "final-project.html#timeline",
    "href": "final-project.html#timeline",
    "title": "Final Project",
    "section": "Timeline",
    "text": "Timeline\nWe’re going to build up to the final project gradually over the whole quarter. These assignments are ungraded. Their purpose is to give you structure and get you instructor feedback early and often.\n\n\n\n\n\n\n\nWeek\nAssignment\n\n\n\n\n2\nChoose data and question\n\n\n4\nExploratory analysis\n\n\n7\nDescribe hypotheses with text and visuals\n\n\n8\nDescribe model in statistical notation and fit model to simulated data\n\n\n9\nFit model to real world data\n\n\n10\nFirst draft of blog post due Monday 12/1\n\n\nExam\nFinal draft of blog post due Thursday 12/11"
  },
  {
    "objectID": "final-project.html#model-and-response-options",
    "href": "final-project.html#model-and-response-options",
    "title": "Final Project",
    "section": "Model and response options",
    "text": "Model and response options\nThe following list of models are suggestions for your final project together with the response characteristics they’re designed for. You are allowed to choose a model not on this list if you’re motivated to do so, but make sure to clear it with the instructors. The models are categorized by complexity level so you can choose a model aligned with your learning curve. You’re encouraged to discuss options with the instructors!\n\n\n\n\n\n\n\n\n\nComplexity level\nModel name\nResponse characteristics\nExample\n\n\n\n\nFamiliar\nGamma\nPositive continuous\nPollution concentrations\n\n\nNegative binomial\nCounts\nWater quality violations\n\n\nBeta\nProportions\nAlgae coverage on a reef\n\n\nMore complex\nHurdle\nInflated zeroes\nOccurrence of rare species\n\n\nSegmented\nBreakpoint\nPolicy intervention\n\n\nMultinomial\nUnordered categories\nLand cover types\n\n\nMost complex\nOrdinal\nOrdered categories\nLikert-scale surveys\n\n\nState-space\nTime series\nSalmon returns\n\n\nSpatial error\nSpatially autocorrelated\nEnergy costs in counties"
  },
  {
    "objectID": "final-project.html#specifications",
    "href": "final-project.html#specifications",
    "title": "Final Project",
    "section": "Specifications",
    "text": "Specifications\n\nQuestion and data\n\nThe blog post provides context and background for the question\nThe data are explained using text and figures\nThe relationships and causal relationships are described with a DAG\n\n\n\nStatistical model\n\nThe statistical model is explained conceptually and using formal statistical notation\nThe blog post demonstrates how to simulate data according to model assumptions\nThe blog post demonstrates that a model fit to the simulated data recovers the parameters\n\n\n\nInference\n\nHypotheses are stated in plain language and with visualizations\nModel estimates are presented with appropriate uncertainty (e.g., confidence intervals)\nA hypothesis is tested and the evidence is interpreted\n\n\n\nProfessionalism\n\nThe overall appearance of the blog post (e.g., figures, code outputs) is portfolio-quality\nThe writing is comprehensible to a technical audience\nThe code is well-organized and appropriately documented"
  },
  {
    "objectID": "course-materials/keys/key1.1.html",
    "href": "course-materials/keys/key1.1.html",
    "title": "Lab 1.1 KEY",
    "section": "",
    "text": "# load packages ----\nlibrary(tidyverse)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "course-materials/keys/key1.1.html#setup",
    "href": "course-materials/keys/key1.1.html#setup",
    "title": "Lab 1.1 KEY",
    "section": "",
    "text": "# load packages ----\nlibrary(tidyverse)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "course-materials/keys/key1.1.html#tidy-data-review",
    "href": "course-materials/keys/key1.1.html#tidy-data-review",
    "title": "Lab 1.1 KEY",
    "section": "Tidy Data Review",
    "text": "Tidy Data Review\nExample untidy / wide data:\n\n# create some untidy temperature data ----\ntemp_data_wide &lt;- tribble(\n  ~date, ~station1, ~station2,  ~station3,\n  \"2023-10-01\", 30.1, 29.8,  31.2,\n  \"2023-11-01\", 28.6, 29.1,  33.4,\n  \"2023-12-01\", 29.9, 28.5,  32.3\n)\n\n# print it out ----\nprint(temp_data_wide)\n\n# A tibble: 3 × 4\n  date       station1 station2 station3\n  &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 2023-10-01     30.1     29.8     31.2\n2 2023-11-01     28.6     29.1     33.4\n3 2023-12-01     29.9     28.5     32.3\n\n\nUsing pivot_longer() to “lengthen” / tidy our data:\n\n# convert data from wide &gt; long ----\ntemp_data_long &lt;- temp_data_wide |&gt; \n  pivot_longer(cols = starts_with(\"station\"),\n               names_to = \"station_id\",\n               values_to = \"temp_c\")\n\n# print it out ----\nprint(temp_data_long)\n\n# A tibble: 9 × 3\n  date       station_id temp_c\n  &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt;\n1 2023-10-01 station1     30.1\n2 2023-10-01 station2     29.8\n3 2023-10-01 station3     31.2\n4 2023-11-01 station1     28.6\n5 2023-11-01 station2     29.1\n6 2023-11-01 station3     33.4\n7 2023-12-01 station1     29.9\n8 2023-12-01 station2     28.5\n9 2023-12-01 station3     32.3"
  },
  {
    "objectID": "course-materials/labs/lab1.html",
    "href": "course-materials/labs/lab1.html",
    "title": "Lab 1",
    "section": "",
    "text": "In today’s lab you will…\n\nNumerically and graphically explore categorical data\nNumerically and graphically explore continuous data\nExplore a real world dataset"
  },
  {
    "objectID": "course-materials/labs/lab1.html#learning-objectives",
    "href": "course-materials/labs/lab1.html#learning-objectives",
    "title": "Lab 1",
    "section": "",
    "text": "In today’s lab you will…\n\nNumerically and graphically explore categorical data\nNumerically and graphically explore continuous data\nExplore a real world dataset"
  },
  {
    "objectID": "course-materials/labs/lab1.html#setup",
    "href": "course-materials/labs/lab1.html#setup",
    "title": "Lab 1",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\ntheme_set(theme_bw(18))\n\nWe’ll use the penguins dataset for today’s lab. Refresh your memory of what it looks like with glimpse().\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…"
  },
  {
    "objectID": "course-materials/labs/lab1.html#categorical-data",
    "href": "course-materials/labs/lab1.html#categorical-data",
    "title": "Lab 1",
    "section": "Categorical data",
    "text": "Categorical data\nCategorical data fall into discrete categories. Examples include:\n\nLand cover type (e.g., forest, wetland, urban)\nEnergy source (e.g., solar, wind, natural gas, coal)\nAir pollutant (e.g., common, hazardous, particulate matter)\n\nWe usually summarize categorical data using counts and proportions. Here we’ll explore how to do so with tables and figures.\n\nContingency tables\nContingency tables summarize combinations of variables by frequency. Create one using the base R function table(). The table below reveals an important pattern not apparent using glimpse(): not all species of penguins were observed at all islands.\n\ntable(penguins$species, penguins$island)\n\n           \n            Biscoe Dream Torgersen\n  Adelie        44    56        52\n  Chinstrap      0    68         0\n  Gentoo       124     0         0\n\n\nThe tidyverse count() function will give you a contingency table in long format.\n\npenguins %&gt;% \n  count(species, island)\n\n# A tibble: 5 × 3\n  species   island        n\n  &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;\n1 Adelie    Biscoe       44\n2 Adelie    Dream        56\n3 Adelie    Torgersen    52\n4 Chinstrap Dream        68\n5 Gentoo    Biscoe      124\n\n\nQ1 How would you pivot it to look like the contingency table above?\nWe often modify contingency tables in two ways.\nFirst, we often add marginal totals, i.e., the total of each row and column. The addmargins() function does that for us.\n\nspecies_island_table &lt;- table(penguins$species, penguins$island)\naddmargins(species_island_table)\n\n           \n            Biscoe Dream Torgersen Sum\n  Adelie        44    56        52 152\n  Chinstrap      0    68         0  68\n  Gentoo       124     0         0 124\n  Sum          168   124        52 344\n\n\nThese marginal totals demonstrate how counts can be difficult to compare, if they come from rows or columns with different totals. For example, more Adélie penguins were observed at Dream than Torgersen, but they represent a smaller fraction of the observations. Normalizing by proportion helps clarify that. You can calculate proportions either by row or column, for which we can use prop.table().\n\n# Row proportions\nprop.table(species_island_table, margin = 1)\n\n           \n               Biscoe     Dream Torgersen\n  Adelie    0.2894737 0.3684211 0.3421053\n  Chinstrap 0.0000000 1.0000000 0.0000000\n  Gentoo    1.0000000 0.0000000 0.0000000\n\n# Column proportions\nprop.table(species_island_table, margin = 2)\n\n           \n               Biscoe     Dream Torgersen\n  Adelie    0.2619048 0.4516129 1.0000000\n  Chinstrap 0.0000000 0.5483871 0.0000000\n  Gentoo    0.7380952 0.0000000 0.0000000\n\n\nQ2 How would you use count(), group_by(), mutate(), and pivoting to create row- and column-wise proportion tables?\n\n\nBar plots\nBar plots are effective for visualizing counts and proportions of one or more categories.\nThis bar plot visualizes the counts of each species.\n\nggplot(penguins, aes(species)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nQ3 How would you use another aesthetic to show the composition of each species by island?\nQ4 Contrast the bar plot from Q3 with the contingency table from the beginning of the lab - how do they convey information differently?"
  },
  {
    "objectID": "course-materials/labs/lab1.html#continuous-data",
    "href": "course-materials/labs/lab1.html#continuous-data",
    "title": "Lab 1",
    "section": "Continuous data",
    "text": "Continuous data\nContinuous data can take any value within their bounds. Examples include:\n\nUnbounded data (e.g., temperature1)\nBounded on one side (e.g., time-to-failure can only be positive)\nBounded on two sides (e.g., fractional cover of an invasive plant in a quadrat)\n\nWe can graphically depict continuous data as one variable (i.e., its distribution) or the relationship between two variables.\n\nHistograms and shape\nHistograms show the frequency of observations within bins, revealing the shape of the variable’s distribution.\n\npenguins %&gt;% \n  ggplot(aes(body_mass_g)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nQ5 What’s the difference between a histogram and a bar plot?\n\n\nMean and standard deviation\nRecall that parameters and statistics are numerical summaries of populations and samples, respectively. Mean and standard deviation are useful summaries, but be careful as they can hide important differences.\nThe mean is a measure of central tendency i.e., the expected or center value of an observation in the sample. The standard deviation is a measure of variation i.e., how spread out the sample is.\nAs a rule of thumb, about 68% of a sample’s data will fall within one standard deviation of the mean, and about 95% will fall within two standard deviations. The more “normally” distributed the data is (more on that tomorrow) the better these rules of thumb hold. Highly skewed data or data with a lot of outliers will deviate more from the rule of thumb.\nQ6 What percentage of penguin body masses fall within one standard deviation of the mean? Within two?\n\n\nScatterplots for paired data\nScatter plots depict relationships between variables.\n\npenguins %&gt;% \n  ggplot(aes(bill_length_mm, bill_depth_mm)) +\n  geom_point() \n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe scatterplot above suggests little relationship between bill length and depth, or perhaps a slightly negative correlation. That means longer bills are not associated with deeper bills. However, if you ask a penguin anatomist, they’ll tell you the opposite is true: longer bills tend to be deeper, too. How can we reconcile these contrasting statements?\nThis is an example of Simpson’s paradox, which is when between-group correlations of one kind mask within-group correlations of another kind. Unmask the paradox by color-coding the points by species.\nQ7: Modify the scatterplot above to color-code the points by species. Then describe the correlations between bill depth, bill length, and species. How do the correlations across species mask the correlations within?"
  },
  {
    "objectID": "course-materials/labs/lab1.html#where-to-find-data",
    "href": "course-materials/labs/lab1.html#where-to-find-data",
    "title": "Lab 1",
    "section": "Where to find data",
    "text": "Where to find data\nThe Yale University Library provides a useful guide for where to find environmental data. The categories include:\n\nGeneral Environmental Data\nEnergy Data\nClimate & Atmospheric Data\nAgricultural, Forestry, and Ecology Data\nHealth Related Data\n\nFind a dataset containing multiple categorical and continuous variables. You’ll have an easier time if you limit your search to tabular data. Explore the data using each of the following tools.\n\nContingency table\nBar plot\nHistogram\nScatterplot"
  },
  {
    "objectID": "course-materials/labs/lab1.html#finished",
    "href": "course-materials/labs/lab1.html#finished",
    "title": "Lab 1",
    "section": "Finished?",
    "text": "Finished?\nIf you wrap up before lab is over, head over to the final project page on the course website and look at the section Model and response options. Your final project will require you to choose a question and data to investigate using a model we won’t teach you in class. The defining characteristic for these models are the response variable they can handle. Start thinking up a question and looking for data where the response variable has characteristics listed in the Model and response options table. Keep in mind that transformations and merging datasets are both options, so even if the raw data you find don’t match one of the eligible models, you may be able to derive something that does. Happy hunting!"
  },
  {
    "objectID": "course-materials/labs/lab1.html#footnotes",
    "href": "course-materials/labs/lab1.html#footnotes",
    "title": "Lab 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes, absolute zero does exist, but practically for our purposes temperature is unbounded↩︎"
  },
  {
    "objectID": "course-materials/labs/lab4.html",
    "href": "course-materials/labs/lab4.html",
    "title": "Lab 4",
    "section": "",
    "text": "In today’s lab you will…\n\nRun a permutation test to answer a “yes/no?” question\nBootstrap a confidence interval to answer a “how much?” question\n\n\nlibrary(tidyverse)\ntheme_set(theme_classic(14))\nset.seed(123)"
  },
  {
    "objectID": "course-materials/labs/lab4.html#learning-objectives",
    "href": "course-materials/labs/lab4.html#learning-objectives",
    "title": "Lab 4",
    "section": "",
    "text": "In today’s lab you will…\n\nRun a permutation test to answer a “yes/no?” question\nBootstrap a confidence interval to answer a “how much?” question\n\n\nlibrary(tidyverse)\ntheme_set(theme_classic(14))\nset.seed(123)"
  },
  {
    "objectID": "course-materials/labs/lab4.html#eel-grass-restoration",
    "href": "course-materials/labs/lab4.html#eel-grass-restoration",
    "title": "Lab 4",
    "section": "Eel grass restoration",
    "text": "Eel grass restoration\nRecall the eel grass restoration example from yesterday’s lecture. The data recorded whether attempts to restore eel grass were successful based on the method used (garden staple or popsicle stick) Figure 1.\n\nrestoration &lt;- read_csv(\"data/eelgrass.csv\",\n                        show_col_types = FALSE)\nglimpse(restoration)\n\nRows: 100\nColumns: 5\n$ plot_id          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ treatment        &lt;chr&gt; \"Garden staple\", \"Popsicle stick\", \"Garden staple\", \"…\n$ success          &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,…\n$ shoot_density_m2 &lt;dbl&gt; 45.94241, 91.60585, 120.92760, 67.86314, 88.09977, 87…\n$ success_fct      &lt;chr&gt; \"Succeed\", \"Succeed\", \"Fail\", \"Fail\", \"Succeed\", \"Suc…\n\n\n\nggplot(restoration, aes(treatment, fill = success_fct)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"firebrick\", \"cornflowerblue\")) +\n  labs(x = \"Restoration method\",\n       y = \"Sites\",\n       fill = \"Outcome\") +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(1, 1),\n        legend.justification = c(1, 1))\n\n\n\n\n\n\n\nFigure 1: Success of eelgrass restoration plots by restoration method."
  },
  {
    "objectID": "course-materials/labs/lab4.html#permutation-test",
    "href": "course-materials/labs/lab4.html#permutation-test",
    "title": "Lab 4",
    "section": "Permutation test",
    "text": "Permutation test\nWe want to know if the popsicle stick method works better for restoration than garden staples. This is a “yes/no?” question, so we’ll use a permutation test for our hypothesis. Recall the steps for hypothesis testing:\n\nIdentify the TEST STATISTIC\nState your NULL and ALTERNATIVE hypotheses\nCalculate the OBSERVED test statistic\nEstimate the NULL DISTRIBUTION\nCalculate P-VALUE\nCompare p-value to CRITICAL THRESHOLD\n\n\nIdentify the test statistic\nQ1: What is the appropriate test statistic for this question?\n\n\nState your null and alternative hypotheses\nQ2: What are your null and alternative hypotheses?\nH0:\nHA:\n\n\nCalculate the observed test statistic\nQ3: How would you calculate the test statistic for the sample?\n\n\nEstimate the null distribution\nThis is the key part of a permutation test! Remember, our goal is to estimate the distribution of possible outcomes under the null hypothesis. To do that, we have to break the association between treatment and outcome.\nQ4: What column should we shuffle to break the association between treatment and outcome?\nQ5: Fill in the following code to perform one permutation and calculate the test statistic.\n\none_permutation &lt;- ??? %&gt;% \n  mutate(??? = sample(???, \n                      size = length(???), \n                      replace = FALSE))\n\npermutation_props &lt;- one_permutation %&gt;% \n  group_by(???) %&gt;% \n  summarize(???)\n\npermutation_diff_props &lt;- permutation_props$???[2] - permutation_props$???[1]\n\npermutation_diff_props\n\nThat gives us the value of the test statistic for just one permutation. To get a distribution, we have to repeat the process many times. Let’s do it 1,000 times.\nQ6: Fill in the following code to perform 1,000 permutations and estimate the null distribution\n\npermute &lt;- function(i) {\n  ???\n}\n\nnull_distribution &lt;- map_dbl(???, ???)\n\nQ7: Visualize the null distribution using a histogram and show where the observed test statistic falls\n\n\nCalculate the p-value\nThe p-value is the probability of a test statistic at least as extreme as the observed, given the null hypothesis. In other words, what proportion of the null distribution exceeds the observed?\nQ8: Calculate the p-value using the null distribution and observed test statistic\n\np_val &lt;- ???\n\np_val\n\n\n\nInterpret the p-value\nWhen interpreting the p-value, we compare it to a critical threshold, usually denoted with \\(\\alpha\\). By convention, we usually set \\(\\alpha\\) to 0.05.\nQ10: Given \\(p \\gt \\alpha\\), which of the following statements is a correct interpretation and why?\n\nOur evidence is consistent with the hypothesis that restoration method does not influence restoration outcome\nWe cannot reject the hypothesis that restoration method does not influence restoration outcome"
  },
  {
    "objectID": "course-materials/labs/lab4.html#bootstrap-confidence-interval",
    "href": "course-materials/labs/lab4.html#bootstrap-confidence-interval",
    "title": "Lab 4",
    "section": "Bootstrap confidence interval",
    "text": "Bootstrap confidence interval\nNow let’s answer a “how much?” question. We want to estimate an interval that we think contains the population parameter. For that, we use bootstrapping.\nRecall the steps for bootstrapping:\n\nIdentify the TEST STATISTIC\nSubstitute sample for population and draw BOOTSTRAP SAMPLES\nEstimate the BOOTSTRAP DISTRIBUTION of the test statistic\nCalculate the CONFIDENCE INTERVAL\n\n\nIdentify the test statistic\nQ11: What is the appropriate test statistic for this question?\n\n\nDraw bootstrap samples\nThis is the key part of bootstrapping! Remember, our goal is to estimate the variation of our population’s parameter due to sampling. To do that, we simulate the process of re-doing our experiment, using the original sample as a substitute for the population. To “re-do” our experiment, we have to keep the association between treatment and outcome.\nQ12: Fill in the following code to draw one bootstrap sample.\n\none_bootstrap &lt;- ??? %&gt;% \n  ??? %&gt;% \n  mutate(??? = sample(???,\n                      size = length(???),\n                      replace = ???)) %&gt;% \n  ungroup()\n\nQ13: Fill in the following code to draw 1,000 bootstrap samples.\n\n\n\n\n\n\nTip\n\n\n\nlist_rbind() will take a list of data frames and bind them row-wise into one data frame.\n\n\n\nbootstrap &lt;- function(i) {\n  ??? %&gt;% \n    mutate(trial = i)\n}\n\nbootstrap_samples &lt;- map(???, ???) %&gt;% \n  ???\n\n\n\nEstimate the bootstrap distribution of the test statistic\nQ14: Fill in the following code to calculate the test statistic for each bootstrap sample.\nQ15: Visualize the bootstrapped distribution of the test statistic.\n\n\nCalculate the confidence interval\nA confidence interval (CI) is a range we are confident contains the population parameter. The bootstrapped distribution of the test statistic describes where we expect the population parameter to fall. So a 95% confidence interval, for example, spans the range from the 2.5% quantile of the bootstrap distribution to the 97.5% quantile.\nQ16: Find the bounds of the 95% CI.\n\n\n\n\n\n\nTip\n\n\n\nThe quantile() function finds quantiles. It’s vectorized over the parameter probs, so you can find multiple quantiles at once\n\n\n\nrestoration_ci &lt;- ???\nrestoration_ci\n\nQ17: Update your visual from Q15 to include the observed test statistic with a solid line and the confidence interval represented with dotted lines."
  },
  {
    "objectID": "course-materials/labs/lab4.html#permutation-vs-bootstrap",
    "href": "course-materials/labs/lab4.html#permutation-vs-bootstrap",
    "title": "Lab 4",
    "section": "Permutation vs bootstrap",
    "text": "Permutation vs bootstrap\nThe visualization you created for Q7 shows the null distribution of the test statistic. The visualization you created for Q17 shows the bootstrapped distribution of the test statistic.\nQ18: What did you do to make the null distribution center on zero? Specifically, what code?\nQ19: What did you do to make the bootstrap distribution center on the observed test statistic? Specifically, what code?\nQ20: What would happen to your bootstrap distribution if you sampled without replacement?"
  },
  {
    "objectID": "course-materials/labs/lab5.html",
    "href": "course-materials/labs/lab5.html",
    "title": "Lab 4",
    "section": "",
    "text": "In today’s lab you will…\n\nRun a permutation test to answer a “yes/no?” question\nBootstrap a confidence interval to answer a “how much?” question\n\n\nlibrary(tidyverse)\ntheme_set(theme_classic(14))\nset.seed(123)"
  },
  {
    "objectID": "course-materials/labs/lab5.html#learning-objectives",
    "href": "course-materials/labs/lab5.html#learning-objectives",
    "title": "Lab 4",
    "section": "",
    "text": "In today’s lab you will…\n\nRun a permutation test to answer a “yes/no?” question\nBootstrap a confidence interval to answer a “how much?” question\n\n\nlibrary(tidyverse)\ntheme_set(theme_classic(14))\nset.seed(123)"
  },
  {
    "objectID": "course-materials/labs/lab5.html#eel-grass-restoration",
    "href": "course-materials/labs/lab5.html#eel-grass-restoration",
    "title": "Lab 4",
    "section": "Eel grass restoration",
    "text": "Eel grass restoration\nRecall the eel grass restoration example from yesterday’s lecture. The data recorded whether attempts to restore eel grass were successful based on the method used (garden staple or popsicle stick) Figure 1.\n\nrestoration &lt;- read_csv(\"data/eelgrass.csv\",\n                        show_col_types = FALSE)\nglimpse(restoration)\n\nRows: 100\nColumns: 5\n$ plot_id          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ treatment        &lt;chr&gt; \"Garden staple\", \"Popsicle stick\", \"Garden staple\", \"…\n$ success          &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,…\n$ shoot_density_m2 &lt;dbl&gt; 45.94241, 91.60585, 120.92760, 67.86314, 88.09977, 87…\n$ success_fct      &lt;chr&gt; \"Succeed\", \"Succeed\", \"Fail\", \"Fail\", \"Succeed\", \"Suc…\n\n\n\nggplot(restoration, aes(treatment, fill = success_fct)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"firebrick\", \"cornflowerblue\")) +\n  labs(x = \"Restoration method\",\n       y = \"Sites\",\n       fill = \"Outcome\") +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(1, 1),\n        legend.justification = c(1, 1))\n\n\n\n\n\n\n\nFigure 1: Success of eelgrass restoration plots by restoration method."
  },
  {
    "objectID": "course-materials/labs/lab5.html#permutation-test",
    "href": "course-materials/labs/lab5.html#permutation-test",
    "title": "Lab 4",
    "section": "Permutation test",
    "text": "Permutation test\nWe want to know if the popsicle stick method works better for restoration than garden staples. This is a “yes/no?” question, so we’ll use a permutation test for our hypothesis. Recall the steps for hypothesis testing:\n\nIdentify the TEST STATISTIC\nState your NULL and ALTERNATIVE hypotheses\nCalculate the OBSERVED test statistic\nEstimate the NULL DISTRIBUTION\nCalculate P-VALUE\nCompare p-value to CRITICAL THRESHOLD\n\n\nIdentify the test statistic\nQ1: What is the appropriate test statistic for this question?\n\n\nState your null and alternative hypotheses\nQ2: What are your null and alternative hypotheses?\nH0:\nHA:\n\n\nCalculate the observed test statistic\nQ3: How would you calculate the test statistic for the sample?\n\n\nEstimate the null distribution\nThis is the key part of a permutation test! Remember, our goal is to estimate the distribution of possible outcomes under the null hypothesis. To do that, we have to break the association between treatment and outcome.\nQ4: What column should we shuffle to break the association between treatment and outcome?\nQ5: Fill in the following code to perform one permutation and calculate the test statistic.\n\none_permutation &lt;- ??? %&gt;% \n  mutate(??? = sample(???, \n                      size = length(???), \n                      replace = FALSE))\n\npermutation_props &lt;- one_permutation %&gt;% \n  group_by(???) %&gt;% \n  summarize(???)\n\npermutation_diff_props &lt;- permutation_props$???[2] - permutation_props$???[1]\n\npermutation_diff_props\n\nThat gives us the value of the test statistic for just one permutation. To get a distribution, we have to repeat the process many times. Let’s do it 1,000 times.\nQ6: Fill in the following code to perform 1,000 permutations and estimate the null distribution\n\npermute &lt;- function(i) {\n  ???\n}\n\nnull_distribution &lt;- map_dbl(???, ???)\n\nQ7: Visualize the null distribution using a histogram and show where the observed test statistic falls\n\n\nCalculate the p-value\nThe p-value is the probability of a test statistic at least as extreme as the observed, given the null hypothesis. In other words, what proportion of the null distribution exceeds the observed?\nQ8: Calculate the p-value using the null distribution and observed test statistic\n\np_val &lt;- ???\n\np_val\n\n\n\nInterpret the p-value\nWhen interpreting the p-value, we compare it to a critical threshold, usually denoted with \\(\\alpha\\). By convention, we usually set \\(\\alpha\\) to 0.05.\nQ10: Given \\(p \\gt \\alpha\\), which of the following statements is a correct interpretation and why?\n\nOur evidence is consistent with the hypothesis that restoration method does not influence restoration outcome\nWe cannot reject the hypothesis that restoration method does not influence restoration outcome"
  },
  {
    "objectID": "course-materials/labs/lab5.html#bootstrap-confidence-interval",
    "href": "course-materials/labs/lab5.html#bootstrap-confidence-interval",
    "title": "Lab 4",
    "section": "Bootstrap confidence interval",
    "text": "Bootstrap confidence interval\nNow let’s answer a “how much?” question. We want to estimate an interval that we think contains the population parameter. For that, we use bootstrapping.\nRecall the steps for bootstrapping:\n\nIdentify the TEST STATISTIC\nSubstitute sample for population and draw BOOTSTRAP SAMPLES\nEstimate the BOOTSTRAP DISTRIBUTION of the test statistic\nCalculate the CONFIDENCE INTERVAL\n\n\nIdentify the test statistic\nQ11: What is the appropriate test statistic for this question?\n\n\nDraw bootstrap samples\nThis is the key part of bootstrapping! Remember, our goal is to estimate the variation of our population’s parameter due to sampling. To do that, we simulate the process of re-doing our experiment, using the original sample as a substitute for the population. To “re-do” our experiment, we have to keep the association between treatment and outcome.\nQ12: Fill in the following code to draw one bootstrap sample.\n\none_bootstrap &lt;- ??? %&gt;% \n  ??? %&gt;% \n  mutate(??? = sample(???,\n                      size = length(???),\n                      replace = ???)) %&gt;% \n  ungroup()\n\nQ13: Fill in the following code to draw 1,000 bootstrap samples.\n\n\n\n\n\n\nTip\n\n\n\nlist_rbind() will take a list of data frames and bind them row-wise into one data frame.\n\n\n\nbootstrap &lt;- function(i) {\n  ??? %&gt;% \n    mutate(trial = i)\n}\n\nbootstrap_samples &lt;- map(???, ???) %&gt;% \n  ???\n\n\n\nEstimate the bootstrap distribution of the test statistic\nQ14: Fill in the following code to calculate the test statistic for each bootstrap sample.\nQ15: Visualize the bootstrapped distribution of the test statistic.\n\n\nCalculate the confidence interval\nA confidence interval (CI) is a range we are confident contains the population parameter. The bootstrapped distribution of the test statistic describes where we expect the population parameter to fall. So a 95% confidence interval, for example, spans the range from the 2.5% quantile of the bootstrap distribution to the 97.5% quantile.\nQ16: Find the bounds of the 95% CI.\n\n\n\n\n\n\nTip\n\n\n\nThe quantile() function finds quantiles. It’s vectorized over the parameter probs, so you can find multiple quantiles at once\n\n\n\nrestoration_ci &lt;- ???\nrestoration_ci\n\nQ17: Update your visual from Q15 to include the observed test statistic with a solid line and the confidence interval represented with dotted lines."
  },
  {
    "objectID": "course-materials/labs/lab5.html#permutation-vs-bootstrap",
    "href": "course-materials/labs/lab5.html#permutation-vs-bootstrap",
    "title": "Lab 4",
    "section": "Permutation vs bootstrap",
    "text": "Permutation vs bootstrap\nThe visualization you created for Q7 shows the null distribution of the test statistic. The visualization you created for Q17 shows the bootstrapped distribution of the test statistic.\nQ18: What did you do to make the null distribution center on zero? Specifically, what code?\nQ19: What did you do to make the bootstrap distribution center on the observed test statistic? Specifically, what code?\nQ20: What would happen to your bootstrap distribution if you sampled without replacement?"
  }
]